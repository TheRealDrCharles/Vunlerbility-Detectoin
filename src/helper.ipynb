{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd016927-a836-44a9-b572-26d5946b74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data from /home/swj/VD/data/ALL/VLC/....\n",
      "[INFO] The length of the loaded data list is : 6159\n",
      "[INFO] Perform tokenization ....\n",
      "[INFO] Tokenization completed!\n",
      "[INFO] -------------------------------------------------------\n",
      "[INFO] Perform code embedding ....\n",
      "----------------------------------------\n",
      "Start training the Word2Vec model. Please wait.. \n",
      "Model training completed!\n",
      "----------------------------------------\n",
      "The trained word2vec model: \n",
      "Word2Vec(vocab=623, vector_size=100, alpha=0.025)\n",
      "-------------------------------------------------------\n",
      "Loading trained Word2vec model. \n",
      "The trained word2vec model: \n",
      "<_io.TextIOWrapper name='embedding/w2v_model.txt' mode='r' encoding='UTF-8'>\n",
      "Found 624 word vectors.\n",
      "[INFO] Word2vec loaded! \n",
      "[INFO] Pad the sequence to unified length...\n",
      "[INFO] Patition the data ....\n",
      "[INFO] Data Shapes Are:\n",
      "[INFO] Data processing completed!\n",
      "[INFO] -------------------------------------------------------\n",
      "[INFO] There are 4927 total samples in the training set. tensor(34, device='cuda:0') vulnerable samples. \n",
      "[INFO] There are 1232 total samples in the test set. tensor(10, device='cuda:0') vulnerable samples. \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from src.embedding import WordToVec as Embedding_Model\n",
    "from src.DataLoader import LoadToken, GenerateLabels, LoadPickleData, SavedPickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from src.plstm_cell import PLSTM\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "\n",
    "tokenizer_saved_path = 'embedding/'\n",
    "repre_saved_path = 'Representation/'\n",
    "data_path = 'data/ALL/VLC/'\n",
    "test_size = 0.2\n",
    "validation_size = 0.2\n",
    "config = yaml.safe_load(open('config/config.yaml', 'r'))\n",
    "random_seed = config['embedding_settings']['seed']\n",
    "def verbose(msg):\n",
    "    ''' Verbose function for print information to stdout'''\n",
    "    print('[INFO]', msg)\n",
    "    \n",
    "def loadData(data_path):\n",
    "    ''' Load data for training/validation'''\n",
    "    verbose('Loading data from '+ os.getcwd() + os.sep + data_path + '....')\n",
    "    total_list, total_list_id = LoadToken(data_path)\n",
    "    verbose(\"The length of the loaded data list is : \" + str(len(total_list)))\n",
    "    return total_list, total_list_id\n",
    "\n",
    "def Tokenization(data_list):\n",
    "    tokenizer = Tokenizer(num_words=None, filters=',', lower=False, char_level=False, oov_token=None) \n",
    "    tokenizer.fit_on_texts(data_list)\n",
    "    # Save the tokenizer.\n",
    "    with open(tokenizer_saved_path + 'tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle)\n",
    "    \n",
    "def LoadToknizer(path_of_tokenizer):\n",
    "    tokenizer = LoadPickleData(path_of_tokenizer)\n",
    "    return tokenizer\n",
    "\n",
    "def padding(sequences_to_pad):\n",
    "    padded_seq = pad_sequences(sequences_to_pad, maxlen = config['model_settings']['model_para']['max_sequence_length'], padding ='post')\n",
    "    return padded_seq\n",
    "\n",
    "def patitionData(data_list_pad, data_list_id):\n",
    "    \n",
    "    test_size = config['training_settings']['dataset_config']['Test_set_ratio']\n",
    "    validation_size = config['training_settings']['dataset_config']['Validation_set_ratio'] \n",
    "    data_list_label = GenerateLabels(data_list_id)\n",
    "        \n",
    "    if not config['training_settings']['using_separate_test_set']:\n",
    "        # The value of the seed for testing should be the same to that was used during the training phase.  \n",
    "        train_vali_set_x, test_set_x, train_vali_set_y, test_set_y, train_vali_set_id, test_set_id = train_test_split(data_list_pad, data_list_label, data_list_id, test_size=test_size, random_state=random_seed)\n",
    "#         train_set_x, validation_set_x, train_set_y, validation_set_y, train_set_id, validation_set_id = train_test_split(train_vali_set_x, train_vali_set_y, train_vali_set_id, test_size=validation_size, random_state=random_seed)\n",
    "        \n",
    "        #tuple_with_test = train_set_x, train_set_y, train_set_id, validation_set_x, validation_set_y, validation_set_id, test_set_x, test_set_y, test_set_id\n",
    "        tuple_with_test = train_vali_set_x, train_vali_set_y, test_set_x, test_set_y\n",
    "        return tuple_with_test\n",
    "    else:\n",
    "        train_set_x, validation_set_x, train_set_y, validation_set_y, train_set_id, validation_set_id = train_test_split(train_vali_set_x, train_vali_set_y, train_vali_set_id, test_size=validation_size, random_state=random_seed)\n",
    "        tuple_without_test = train_set_x, train_set_y, train_set_id, validation_set_x, validation_set_y, validation_set_id\n",
    "        return tuple_without_test\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "total_list, total_list_id = loadData(data_path)\n",
    "verbose(\"Perform tokenization ....\")\n",
    "Tokenization(total_list)\n",
    "verbose(\"Tokenization completed!\")\n",
    "verbose(\"-------------------------------------------------------\")   \n",
    "verbose(\"Perform code embedding ....\")\n",
    "embedding_model = Embedding_Model(config)\n",
    "'''total_sequnces: variable lengths of number of samples vectors'''\n",
    "'''word_index: a map of word to vectors'''\n",
    "total_sequences, word_index = embedding_model.LoadTokenizer(total_list)\n",
    "embedding_model.TrainWordToVec(total_list)\n",
    "embedding_matrix, embedding_dim = embedding_model.ApplyWordToVec(word_index)            \n",
    "verbose (\"Word2vec loaded! \")\n",
    "            \n",
    "verbose(\"Pad the sequence to unified length...\")\n",
    "total_list_pad = padding(total_sequences)\n",
    "verbose(\"Patition the data ....\")\n",
    "data_tuple = patitionData(total_list_pad, total_list_id)  \n",
    "        \n",
    "train_set_x = data_tuple[0]\n",
    "train_set_y = np.asarray(data_tuple[1]).flatten()\n",
    "\n",
    "test_set_x = data_tuple[2]\n",
    "test_set_y = np.asarray(data_tuple[3]).flatten()\n",
    "\n",
    "train_set = train_set_x, train_set_y\n",
    "test_set = test_set_x, test_set_y\n",
    "y_test = test_set_y\n",
    "\n",
    "train_set_x = torch.Tensor(train_set_x).long().to(device) # transform to torch tensor\n",
    "train_set_y = torch.Tensor(train_set_y).to(device)\n",
    "train_dataset = TensorDataset(train_set_x,train_set_y) # create your datset\n",
    "train_loader = DataLoader(train_dataset, batch_size=128) # create your dataloader\n",
    "\n",
    "test_set_x = torch.Tensor(test_set_x).long().to(device) # transform to torch tensor\n",
    "test_set_y = torch.Tensor(test_set_y).to(device)\n",
    "test_dataset = TensorDataset(test_set_x,test_set_y) # create your datset\n",
    "test_loader = DataLoader(test_dataset, batch_size=1) # create your dataloader\n",
    "\n",
    "verbose (\"Data Shapes Are:\")\n",
    "verbose (\"Data processing completed!\")\n",
    "verbose (\"-------------------------------------------------------\")\n",
    "verbose (\"There are \" + str(len(train_set_x)) + \" total samples in the training set. \" + str(torch.count_nonzero(train_set_y)) + \" vulnerable samples. \")\n",
    "verbose (\"There are \" + str(len(test_set_x)) + \" total samples in the test set. \" + str(torch.count_nonzero(test_set_y)) + \" vulnerable samples. \")\n",
    "\n",
    "# if config['model_settings']['model_para']['handle_data_imbalance']:\n",
    "#     class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                             torch.unique(train_set_y),\n",
    "#                                             train_set_y)\n",
    "# else:\n",
    "#     class_weights = None\n",
    "        \n",
    "# class_weight_dict = dict(zip([x for x in np.unique(train_set_y)], class_weights))\n",
    "        \n",
    "# verbose (\"-------------------------------------------------------\")\n",
    "\n",
    "weights=torch.from_numpy(embedding_matrix)\n",
    "\n",
    "# train_set_y = torch.FloatTensor(train_set_y).to(device)\n",
    "#has shape of [1, 10000] and [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98247609",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFF_SLOPE=1e-3\n",
    "\n",
    "# function to extract grad\n",
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "    return hook\n",
    "\n",
    "class GradMod(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, other):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        result = torch.fmod(input, other)\n",
    "        ctx.save_for_backward(input, other)        \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        x, y = ctx.saved_variables\n",
    "        return grad_output * 1, grad_output * torch.neg(torch.floor_divide(x, y))\n",
    "class PLSTMCell(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.Periods = nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
    "        self.Shifts = nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
    "        self.On_End = nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
    "        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))\n",
    "        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))\n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        # Phased LSTM\n",
    "        # -----------------------------------------------------\n",
    "        nn.init.constant_(self.On_End, 0.05) # Set to be 5% \"open\"\n",
    "        nn.init.uniform_(self.Shifts, 0, 100) # Have a wide spread of shifts\n",
    "        # Uniformly distribute periods in log space between exp(1, 3)\n",
    "        self.Periods.data.copy_(torch.exp((3 - 1) *\n",
    "            torch.rand(self.Periods.shape) + 1))\n",
    "        # -----------------------------------------------------\n",
    "         \n",
    "    def forward(self, t, x, ts,\n",
    "                init_states=None):\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        if init_states is None:\n",
    "            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), \n",
    "                        torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "\n",
    "        # PHASED LSTM\n",
    "        # -----------------------------------------------------\n",
    "        # Precalculate some useful vars\n",
    "        shift_broadcast = self.Shifts.view(1, -1)\n",
    "        period_broadcast = abs(self.Periods.view(1, -1))\n",
    "        on_mid_broadcast = abs(self.On_End.view(1, -1)) * 0.5 * period_broadcast\n",
    "        on_end_broadcast = abs(self.On_End.view(1, -1)) * period_broadcast                       \n",
    "        \n",
    "        def calc_time_gate(time_input_n):\n",
    "            # Broadcast the time across all units\n",
    "            t_broadcast = time_input_n.unsqueeze(-1)\n",
    "            # Get the time within the period\n",
    "            in_cycle_time = GradMod.apply(t_broadcast + shift_broadcast, period_broadcast)            \n",
    "\n",
    "            # Find the phase\n",
    "            is_up_phase = torch.le(in_cycle_time, on_mid_broadcast)\n",
    "            is_down_phase = torch.gt(in_cycle_time, on_mid_broadcast)*torch.le(in_cycle_time, on_end_broadcast)\n",
    "\n",
    "\n",
    "            # Set the mask\n",
    "            sleep_wake_mask = torch.where(is_up_phase, in_cycle_time/on_mid_broadcast,\n",
    "                                torch.where(is_down_phase,\n",
    "                                    (on_end_broadcast-in_cycle_time)/on_mid_broadcast,\n",
    "                                        OFF_SLOPE*(in_cycle_time/period_broadcast)))\n",
    "            return sleep_wake_mask\n",
    "        # -----------------------------------------------------\n",
    "\n",
    "        HS = self.hidden_size\n",
    "        old_c_t, old_h_t = c_t, h_t\n",
    "        x_t = x[:, t, :]\n",
    "        t_t = ts[:, t]\n",
    "        \n",
    "        # batch the computations into a single matrix multiplication\n",
    "        gates = x_t @ self.W + h_t @ self.U + self.bias\n",
    "        i_t, f_t, g_t, o_t = (\n",
    "            torch.sigmoid(gates[:, :HS]), # input\n",
    "            torch.sigmoid(gates[:, HS:HS*2]), # forget\n",
    "            torch.tanh(gates[:, HS*2:HS*3]),\n",
    "            torch.sigmoid(gates[:, HS*3:]), # output\n",
    "        )\n",
    "        c_t = f_t * c_t + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        # PHASED LSTM\n",
    "        # -----------------------------------------------------\n",
    "        # Get time gate openness\n",
    "        sleep_wake_mask = calc_time_gate(t_t)\n",
    "        # Sleep if off, otherwise stay a bit on\n",
    "        c_t = sleep_wake_mask*c_t + (1. - sleep_wake_mask)*old_c_t\n",
    "        h_t = sleep_wake_mask*h_t + (1. - sleep_wake_mask)*old_h_t\n",
    "        # -----------------------------------------------------\n",
    "        return h_t, c_t\n",
    "\n",
    "class BiPLSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.plstm_cell_forward = PLSTMCell(self.input_sz, self.hidden_size).to(device)\n",
    "        self.plstm_cell_backward = PLSTMCell(self.input_sz, self.hidden_size).to(device)\n",
    "    def forward(self, x, ts,\n",
    "               init_states=None):\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hs_forward = torch.zeros(x.size(0), self.hidden_size).to(device)\n",
    "        cs_forward = torch.zeros(x.size(0), self.hidden_size).to(device)\n",
    "        hs_backward = torch.zeros(x.size(0), self.hidden_size).to(device)\n",
    "        cs_backward = torch.zeros(x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        forward = []\n",
    "        backward = []\n",
    "        \n",
    "        # Forward\n",
    "        for t in range(seq_sz):\n",
    "            hs_forward, cs_forward = self.plstm_cell_forward(t, x, ts, (hs_forward, cs_forward))\n",
    "            forward.append(hs_forward.unsqueeze(0))\n",
    "        # Backward\n",
    "        for t in reversed(range(seq_sz)):\n",
    "            hs_backward, cs_backward = self.plstm_cell_backward(t, x, ts, (hs_backward, cs_backward))\n",
    "            backward.append(hs_backward.unsqueeze(0))\n",
    "        \n",
    "        #doesn't matter actually\n",
    "        h_t, c_t = torch.cat((hs_forward, hs_backward), 1), torch.cat((cs_forward, cs_forward), 1)\n",
    "        \n",
    "        hidden_seq = []\n",
    "        for fwd, bwd in zip(forward, backward):\n",
    "            hidden_seq.append(torch.cat((fwd, bwd), 2))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa26646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
    "        #self.lstm = PLSTM(100, 128).to(device)\n",
    "        self.blstm = BiPLSTM(100, 128).to(device)\n",
    "        self.drop_out1 = nn.Dropout()\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=2)\n",
    "        self.hidden1 = nn.Linear(127*2000, 128)\n",
    "        self.hidden2 = nn.Linear(128,32)\n",
    "        self.hidden3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        ts = x.to(device)\n",
    "        x = self.embedding(x).float()\n",
    "        x, (hn, cn) = self.blstm(x, ts)\n",
    "        x = self.drop_out1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.hidden2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.hidden3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88398464-5ca3-47a7-96c7-e88a087b79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "#torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "print(device)\n",
    "    \n",
    "    \n",
    "net = Net(weights).to(device)\n",
    "# print(weights.shape)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 60\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, epoch_loss, epoch_acc):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        print(f'Epoch {epoch+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    train(net, device, train_loader, optimizer, epoch, epoch_loss, epoch_acc)\n",
    "\n",
    "PATH = 'trained_model/VLC_PLSTM_'\n",
    "torch.save(net.state_dict(), PATH + 'model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d6860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data from /home/swj/VD/data/ALL/VLC/Vul/....\n",
      "[INFO] The length of the loaded data list is : 44\n",
      "torch.Size([44, 2000])\n",
      "-------start_index------------\n",
      "0\n",
      "-------end_index------------\n",
      "44\n",
      "torch.Size([44, 2000])\n",
      "Saving the obtained representations....\n",
      "The obtained representations are saved in: Representation/.\n"
     ]
    }
   ],
   "source": [
    "#obtain representations\n",
    "def ObtainRepresentations_by_batch_size(input_sequences, feature_model, BATCH_SIZE):\n",
    "    num_batches_per_epoch = int((len(input_sequences) - 1) / BATCH_SIZE) + 1\n",
    "    data_size = len(input_sequences)\n",
    "    representations_total = []\n",
    "    model = feature_model\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index = batch_num * BATCH_SIZE\n",
    "        end_index = min((batch_num + 1) * BATCH_SIZE, data_size)\n",
    "        print (\"-------start_index------------\")\n",
    "        print (start_index)\n",
    "        print (\"-------end_index------------\")\n",
    "        print (end_index)\n",
    "        print(input_sequences[start_index: end_index].shape)\n",
    "        representations = model(input_sequences[start_index: end_index])\n",
    "        representations_total = representations_total + representations.tolist()\n",
    "    return np.asarray(representations_total)\n",
    "\n",
    "net = Net(weights).to(device)\n",
    "PATH = 'trained_model/VLC_PLSTM_'\n",
    "model_path = PATH + 'model.h5'\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net.eval()\n",
    "\n",
    "class Feature_Net(nn.Module):\n",
    "    def __init__(self, net, weights):\n",
    "        super(Feature_Net, self).__init__()\n",
    "        self.embedding = net.embedding\n",
    "        self.blstm = net.blstm\n",
    "        self.drop_out1 = net.drop_out1\n",
    "        self.maxpool = net.maxpool\n",
    "        self.hidden1 = net.hidden1\n",
    "        self.relu = net.relu\n",
    "    def forward(self, x):\n",
    "        ts = x.to(device)\n",
    "        x = self.embedding(x).float()\n",
    "        x, (hn, cn) = self.blstm(x, ts)\n",
    "        x = self.drop_out1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        return x\n",
    "\n",
    "feature_model = Feature_Net(net, weights).to(device)\n",
    "feature_model.eval()\n",
    "\n",
    "tokenizer = LoadToknizer(tokenizer_saved_path + 'tokenizer.pickle')\n",
    "data_path_repre = 'data/ALL/VLC/Vul/'\n",
    "data_list, data_list_id = loadData(data_path_repre) \n",
    "data_sequence = tokenizer.texts_to_sequences(data_list)\n",
    "data_list_pad = padding(data_sequence)\n",
    "data_list_pad = torch.Tensor(data_list_pad).long().to(device)\n",
    "print(data_list_pad.shape)\n",
    "obtained_repre = ObtainRepresentations_by_batch_size(data_list_pad, feature_model, 128)\n",
    "print(\"Saving the obtained representations....\") \n",
    "if not os.path.exists(repre_saved_path): os.makedirs(repre_saved_path)\n",
    "SavedPickle(repre_saved_path + \"VLC_Vul.pkl\", obtained_repre)\n",
    "# When the obtained representations are 2-D arrays, we can also save them in a CSV file.\n",
    "#ListToCSV(obtained_repre, self.paras.saved_path)\n",
    "print(\"The obtained representations are saved in: \" + str(repre_saved_path) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700bd30-d08f-4022-9078-39c8fb719743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval the model with only PLSTM used\n",
    "net = Net(weights).to(device)\n",
    "PATH = 'trained_model/FFmpeg_PLSTM_'\n",
    "model_path = PATH + 'model.h5'\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "testloader = test_set\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "predicted_classes = []\n",
    "test_set_y = testloader[1]\n",
    "test_accuracy = 0\n",
    "\n",
    "y_pred_list = []\n",
    "model = net\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        y_test_pred = model(data)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "confusion_matrix(y_test, y_pred_list)\n",
    "print(classification_report(y_test, y_pred_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
