{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa214d-72b5-4003-aa06-d8a3aff013f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'bplstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"command_injection\"\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "\n",
    "#get the vulnerability from the command line argument\n",
    "\n",
    "\n",
    "progress = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "### paramters for the filtering and creation of samples\n",
    "#restriction = [20000,5,6,10] #which samples to filter out\n",
    "restriction = [10000,5,3,5] #which samples to filter out\n",
    "\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "\n",
    "#get word2vec model\n",
    "w2v = embedding_path + \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
    "w2vmodel = w2v + \".model\"\n",
    "\n",
    "#load word2vec model\n",
    "if not (os.path.isfile(w2vmodel)):\n",
    "    print(\"word2vec model is still being created...\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec.load(w2vmodel)\n",
    "print('word2vec model loaded!')\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "#load data\n",
    "with open(data_path + 'plain_' + mode, 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "    \n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"finished loading. \", nowformat)\n",
    "\n",
    "allblocks = []\n",
    "\n",
    "for r in data:\n",
    "    progress = progress + 1\n",
    "\n",
    "    for c in data[r]:\n",
    "\n",
    "        if \"files\" in data[r][c]:                      \n",
    "            if len(data[r][c][\"files\"]) > restriction[3]:\n",
    "                #too many files\n",
    "                continue\n",
    "\n",
    "            for f in data[r][c][\"files\"]:\n",
    "\n",
    "                if len(data[r][c][\"files\"][f][\"changes\"]) >= restriction[2]:\n",
    "                    #too many changes in a single file\n",
    "                    continue\n",
    "\n",
    "                if not \"source\" in data[r][c][\"files\"][f]:\n",
    "                    #no sourcecode\n",
    "                    continue\n",
    "\n",
    "                if \"source\" in data[r][c][\"files\"][f]:\n",
    "                    sourcecode = data[r][c][\"files\"][f][\"source\"]                          \n",
    "                    if len(sourcecode) > restriction[0]:\n",
    "                        #sourcecode is too long\n",
    "                        continue\n",
    "\n",
    "                allbadparts = []\n",
    "\n",
    "                for change in data[r][c][\"files\"][f][\"changes\"]:\n",
    "\n",
    "                    #get the modified or removed parts from each change that happened in the commit                  \n",
    "                    badparts = change[\"badparts\"]\n",
    "                    count = count + len(badparts)\n",
    "\n",
    "                    if len(badparts) > restriction[1]:\n",
    "                        #too many modifications in one change\n",
    "                        break\n",
    "\n",
    "                    for bad in badparts:\n",
    "                        #check if they can be found within the file\n",
    "                        pos = myutils.findposition(bad,sourcecode)\n",
    "                        if not -1 in pos:\n",
    "                            allbadparts.append(bad)\n",
    "\n",
    "                    if (len(allbadparts) > restriction[2]):\n",
    "                        #too many bad positions in the file\n",
    "                        break\n",
    "\n",
    "                if(len(allbadparts) > 0):\n",
    "                    if len(allbadparts) < restriction[2]:\n",
    "                        #find the positions of all modified parts\n",
    "                        positions = myutils.findpositions(allbadparts,sourcecode)\n",
    "\n",
    "                        #get the file split up in samples\n",
    "                        blocks = myutils.getblocks(sourcecode, positions, step, fulllength)\n",
    "\n",
    "                        for b in blocks:\n",
    "                          #each is a tuple of code and label\n",
    "                            allblocks.append(b)\n",
    "\n",
    "keys = []\n",
    "\n",
    "#randomize the sample and split into train, validate and final test set\n",
    "for i in range(len(allblocks)):\n",
    "    keys.append(i)\n",
    "random.shuffle(keys)\n",
    "\n",
    "print('there are total ' + str(len(keys)) + ' files')\n",
    "cutoff = round(0.7 * len(keys)) #     70% for the training set\n",
    "cutoff2 = round(0.85 * len(keys)) #   15% for the validation set and 15% for the final test set\n",
    "\n",
    "keystrain = keys[:cutoff]\n",
    "keystest = keys[cutoff:cutoff2]\n",
    "keysfinaltest = keys[cutoff2:]\n",
    "\n",
    "print(\"cutoff \" + str(cutoff))\n",
    "print(\"cutoff2 \" + str(cutoff2))\n",
    "\n",
    "\n",
    "with open(data_path + mode + '_dataset_keystrain', 'wb') as fp:\n",
    "    pickle.dump(keystrain, fp)\n",
    "with open(data_path + mode + '_dataset_keystest', 'wb') as fp:\n",
    "    pickle.dump(keystest, fp)\n",
    "with open(data_path + mode + '_dataset_keysfinaltest', 'wb') as fp:\n",
    "    pickle.dump(keysfinaltest, fp)\n",
    "\n",
    "TrainX = []\n",
    "TrainY = []\n",
    "ValidateX = []\n",
    "ValidateY = []\n",
    "FinaltestX = []\n",
    "FinaltestY = []\n",
    "\n",
    "\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "codes, lables = [], []\n",
    "for code, lable in allblocks:\n",
    "    codes.append(code.split(' '))\n",
    "    lables.append(lable)\n",
    "Tokenization(codes)\n",
    "total_sequences, word_index = LoadTokenizer(codes)\n",
    "word_vectors = w2v_model.wv\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))#word vectors has dimention 200\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_vectors[words[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "weights=torch.from_numpy(embedding_matrix)\n",
    "with open(embedding_path + mode + '_weights', 'wb') as fp:\n",
    "    pickle.dump(weights, fp)\n",
    "    \n",
    "max_length = fulllength \n",
    "total_sequences = pad_sequences(total_sequences, maxlen=max_length)\n",
    "\n",
    "print(total_sequences.shape)\n",
    "print(\"Creating training dataset... (\" + mode + \")\")\n",
    "for k in keystrain:\n",
    "    TrainX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    TrainY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Train length: \" + str(len(TrainX)))\n",
    "print('saving datasets')\n",
    "with open(data_path + 'plain_' + mode + '_dataset-train-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(TrainX, fp)\n",
    "with open(data_path + 'plain_' + mode + '_dataset-train-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(TrainY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "for k in keystest:\n",
    "    ValidateX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    ValidateY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Test length: \" + str(len(ValidateX)))\n",
    "with open(data_path + 'plain_' + mode + '_dataset-validate-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(ValidateX, fp)\n",
    "with open(data_path + 'plain_' + mode + '_dataset-validate-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(ValidateY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"Creating finaltest dataset...\")\n",
    "for k in keysfinaltest:\n",
    "    FinaltestX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    FinaltestY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Finaltesting length: \" + str(len(FinaltestX)))\n",
    "with open(data_path + mode + '_dataset_finaltest_X', 'wb') as fp:\n",
    "    pickle.dump(FinaltestX, fp)\n",
    "with open(data_path + mode + '_dataset_finaltest_Y', 'wb') as fp:\n",
    "    pickle.dump(FinaltestY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"saved dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009681ce-3b41-4138-986f-87a6c71d5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *#Prepare the data for models\n",
    "model_name = 'plstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"remote_code_execution\"\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "### paramters for the filtering and creation of samples\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "Train_X, Train_y, ValidateX, ValidateY = LoadPickleData(data_path + 'plain_' + mode + '_dataset-train-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-train-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-validate-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-validate-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2)\n",
    "\n",
    "X_train =  np.array(Train_X)\n",
    "y_train =  np.array(Train_y)\n",
    "X_test =  np.array(ValidateX)\n",
    "y_test =  np.array(ValidateY)\n",
    "\n",
    "#in the original collection of data, the 0 and 1 were used the other way round, so now they are switched so that \"1\" means vulnerable and \"0\" means clean.\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0:\n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = 0\n",
    "    \n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"numpy array done. \", nowformat)\n",
    "\n",
    "print(str(len(X_train)) + \" samples in the training set.\")      \n",
    "print(str(len(X_test)) + \" samples in the validation set.\") \n",
    "  \n",
    "csum = 0\n",
    "for a in y_train:\n",
    "    csum = csum+a\n",
    "print(\"percentage of vulnerable samples: \"  + str(int((csum / len(X_train)) * 10000)/100) + \"%\")\n",
    "  \n",
    "testvul = 0\n",
    "for y in y_test:\n",
    "    if y == 1:\n",
    "        testvul = testvul+1\n",
    "print(\"absolute amount of vulnerable samples in test set: \" + str(testvul))\n",
    "\n",
    "  \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"Starting DPLSTM: \", nowformat)\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "train_set_x = torch.Tensor(X_train).long().to(device) # transform to torch tensor\n",
    "train_set_y = torch.Tensor(y_train).to(device)\n",
    "train_dataset = TensorDataset(train_set_x,train_set_y) # create your datset\n",
    "train_loader = DataLoader(train_dataset, batch_size=512) # create your dataloader\n",
    "\n",
    "test_set_x = torch.Tensor(X_test).long().to(device) # transform to torch tensor\n",
    "test_set_y = torch.Tensor(y_test).to(device)\n",
    "test_dataset = TensorDataset(test_set_x,test_set_y) # create your datset\n",
    "test_loader = DataLoader(test_dataset, batch_size=512) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf0063-eeaf-4f14-9abc-9f0a45b55ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'dlstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "code_type = 'python'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "mode = \"remote_code_execution\"\n",
    "model_path = home_path + 'trained_model/' + code_type + '_' + mode + '/'\n",
    "\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "recording_type = 'loss&acc'\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "#torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "print(device)\n",
    "\n",
    "weights=LoadPickleData(embedding_path + mode + '_weights')\n",
    "\n",
    "dlstm = DLSTM_python_net(weights).to(device)\n",
    "plstm = PLSTM_python_net(weights).to(device)\n",
    "blstm = BLSTM_python_net(weights).to(device)\n",
    "lstm = LSTMnet(weights).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = None\n",
    "if model_name == 'blstm':\n",
    "    optimizer = optim.Adam(blstm.parameters(), lr=0.001)\n",
    "elif model_name == 'dlstm':\n",
    "    optimizer = optim.Adam(dlstm.parameters(), lr=0.001)\n",
    "elif model_name == 'plstm':\n",
    "    optimizer = optim.Adam(plstm.parameters(), lr=0.001)\n",
    "elif model_name == 'lstm':\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 60\n",
    "is_first = True\n",
    "def train(model, device, train_loader, test_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        valid_loss += loss.item() \n",
    "        valid_acc += acc.item() \n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(test_loader)}')\n",
    "    return train_loss, train_acc, valid_loss, valid_acc\n",
    "\n",
    "def D_train(model, device, train_loader, test_loader, optimizer, epoch, A):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, A)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "    \n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data, A)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        valid_loss += loss.item() \n",
    "        valid_acc += acc.item() \n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(test_loader)}')\n",
    "    return train_loss, train_acc, valid_loss, valid_acc\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=home_path + 'src/runs/' + model_name + '_' + code_type + '_' + mode + '/', comment=model_name + '_' + code_type + '_' + mode)\n",
    "csv_writer = []\n",
    "\n",
    "ts = torch.rand(train_set_x.shape, dtype=torch.double).to(device)\n",
    "domain_set = len(train_set_x) * [1] + len(ts) * [0]\n",
    "A = torch.eye(200, dtype = train_set_x.dtype).to(device)\n",
    "for epoch in range(epochs):\n",
    "    train_set_x.to(device)\n",
    "    train_set_x = torch.tensor(train_set_x, dtype=torch.double)\n",
    "    A = Newton(torch.cat((train_set_x, ts), dim = 0), domain_set, 4)\n",
    "    ts = ts @ A.sqrt()\n",
    "\n",
    "    time1 = time.time()\n",
    "    if model_name == 'blstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(blstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    elif model_name == 'dlstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(dlstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    elif model_name == 'plstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = D_train(plstm, device, train_loader, test_loader, optimizer, epoch, A)\n",
    "    elif model_name == 'lstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(lstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    time2 = time.time()\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/valid', valid_acc, epoch)\n",
    "    csv_writer.append([epoch, train_loss, valid_loss])\n",
    "    print(f'Epoch {epoch} elapse {time2 - time1}s')\n",
    "\n",
    "Columns = ['epoch','train_loss','valid_loss']\n",
    "CSV_writer=pd.DataFrame(columns=Columns,data=csv_writer)\n",
    "print(CSV_writer)\n",
    "csv_saved_path = home_path + 'results/' + model_name + '/' + code_type + '/' + mode + '/'\n",
    "if not os.path.exists(csv_saved_path): os.makedirs(csv_saved_path)\n",
    "CSV_writer.to_csv(csv_saved_path + model_name + '_' + mode + '.csv',encoding='gbk')\n",
    "\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "if model_name == 'blstm':\n",
    "    torch.save(blstm.state_dict(),  model_path + model_name + '_model.h5')\n",
    "elif model_name == 'dlstm':\n",
    "    torch.save(dlstm.state_dict(), model_path + model_name + '_model.h5')\n",
    "elif model_name == 'lstm':\n",
    "    torch.save(lstm.state_dict(), model_path + model_name + '_model.h5')\n",
    "elif model_name == 'plstm':\n",
    "    torch.save(plstm.state_dict(), model_path + model_name + '_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4877928-f43b-4ad8-b56a-aea95e27ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.includes import *\n",
    "\n",
    "\n",
    "model_name = 'plstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "code_type = 'python'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "mode = \"remote_code_execution\"\n",
    "ways = 'myway'\n",
    "model_path = home_path + 'trained_model/' + code_type + '_' + mode + '/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "repre_saved_path = home_path + 'Representation/' + model_name + '_' + ways + '/' + code_type + '/' + mode + '/'\n",
    "\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "### paramters for the filtering and creation of samples\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "Test_X, Test_y = LoadPickleData(data_path + mode + '_dataset_finaltest_X'), LoadPickleData(data_path + mode + '_dataset_finaltest_Y')\n",
    "\n",
    "X_test =  np.array(Test_X)\n",
    "X_test = torch.Tensor(X_test).long().to(device) # transform to torch tensor\n",
    "\n",
    "    \n",
    "for i in range(len(Test_y)):\n",
    "    if Test_y[i] == 0:\n",
    "        Test_y[i] = 1\n",
    "    else:\n",
    "        Test_y[i] = 0\n",
    "        \n",
    "#Train_X_repre, Train_y_repre, Test_X_repre, Test_y_repre = [], [], [], []\n",
    "#obtain representations\n",
    "def ObtainRepresentations_by_batch_size(input_sequences, feature_model, BATCH_SIZE):\n",
    "    num_batches_per_epoch = int((len(input_sequences) - 1) / BATCH_SIZE) + 1\n",
    "    data_size = len(input_sequences)\n",
    "    representations_total = []\n",
    "    model = feature_model\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index = batch_num * BATCH_SIZE\n",
    "        end_index = min((batch_num + 1) * BATCH_SIZE, data_size)\n",
    "        print (\"-------start_index------------\")\n",
    "        print (start_index)\n",
    "        print (\"-------end_index------------\")\n",
    "        print (end_index)\n",
    "        print(input_sequences[start_index: end_index].shape)\n",
    "        representations = model(input_sequences[start_index: end_index])\n",
    "        representations_total = representations_total + representations.tolist()\n",
    "    return np.asarray(representations_total)\n",
    "\n",
    "weights=LoadPickleData(embedding_path + mode + '_weights')\n",
    "\n",
    "if model_name == 'dlstm':\n",
    "    net = DLSTM_python_net(weights).to(device)\n",
    "elif model_name == 'plstm':\n",
    "    net = PLSTM_python_net(weights).to(device)\n",
    "elif model_name == 'blstm':\n",
    "    net = BLSTM_python_net(weights).to(device)\n",
    "    \n",
    "#change this\n",
    "\n",
    "net.load_state_dict(torch.load(model_path + model_name + '_model.h5'))\n",
    "net.eval()\n",
    "\n",
    "feature_model = None\n",
    "if model_name == 'blstm':\n",
    "    feature_model = BLSTM_python_Feature_Net(net, weights).to(device)\n",
    "elif model_name == 'dlstm':\n",
    "    feature_model = DLSTM_python_Feature_Net(net, weights).to(device)\n",
    "elif model_name == 'plstm':\n",
    "    feature_model = PLSTM_python_Feature_Net(net, weights).to(device)\n",
    "    \n",
    "feature_model.eval()\n",
    "start_time = time.time()\n",
    "if not os.path.exists(repre_saved_path): os.makedirs(repre_saved_path)\n",
    "\n",
    "obtained_repre = ObtainRepresentations_by_batch_size(X_test, feature_model, 512)\n",
    "SavedPickle(repre_saved_path + \"X_test.pkl\", obtained_repre)\n",
    "SavedPickle(repre_saved_path + \"y_test.pkl\", Test_y)\n",
    "print(\"Saving the obtained representations....\") \n",
    "end_time = time.time()\n",
    "print('uisng' + str(end_time - start_time) + 's')\n",
    "print(\"The obtained representations are saved in: \" + str(repre_saved_path) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6250ade-61ac-4117-b7fb-d72f13271fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the num of datasets\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'bplstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"command_injection\"\n",
    "\n",
    "data_sets_names = ['command_injection', 'open_redirect', 'path_disclosure', 'remote_code_execution', 'sql', 'xsrf', 'xss']\n",
    "for data_sets_name in data_sets_names:\n",
    "    data_path = home_path + 'data/python_data_set/' + data_sets_name + '/'\n",
    "    train = LoadPickleData(data_path + 'plain_' + data_sets_name + '_dataset-train-Y_word2vec_withString10-300-200__5_200')\n",
    "    validation = LoadPickleData(data_path + 'plain_' + data_sets_name + '_dataset-validate-Y_word2vec_withString10-300-200__5_200')\n",
    "    test = LoadPickleData(data_path + data_sets_name + '_dataset_finaltest_Y')\n",
    "    \n",
    "    tot = len(train + validation + test)\n",
    "    nonvul = sum(train + validation + test)\n",
    "    vul = tot - nonvul\n",
    "    proportion = vul / tot\n",
    "    print('the data_set is:  ' + data_sets_name + '    there are   ' + str(tot) + ' total samples,  ' + str(vul) + ' vul samples,  ' + str(nonvul) + ' nonvul samples,   ' + 'the propotion is:   ' + str(vul / tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ae18a-ef1f-4846-b273-d81c78e7159c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
