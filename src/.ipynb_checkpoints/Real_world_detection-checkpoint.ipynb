{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0766e4e4-5e70-48ba-9194-ca7d2663b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/swj/VD/data/TEST/C++/VLC/\n",
      "[INFO] Loading data from /home/swj/VD/src//home/swj/VD/data/Six_projects/....\n",
      "[INFO] The length of the loaded data list is : 39944\n",
      "[INFO] Loading data from /home/swj/VD/src//home/swj/VD/data/TEST/C++/VLC/....\n",
      "[INFO] The length of the loaded data list is : 50\n",
      "50\n",
      "[INFO] Perform tokenization ....\n",
      "[INFO] Tokenization completed!\n",
      "[INFO] -------------------------------------------------------\n",
      "[INFO] Perform code embedding ....\n",
      "----------------------------------------\n",
      "Start training the Word2Vec model. Please wait.. \n",
      "Model training completed!\n",
      "----------------------------------------\n",
      "The trained word2vec model: \n",
      "Word2Vec(vocab=990, vector_size=100, alpha=0.025)\n",
      "-------------------------------------------------------\n",
      "Loading trained Word2vec model. \n",
      "The trained word2vec model: \n",
      "<_io.TextIOWrapper name='/home/swj/VD/embedding/w2v_model.txt' mode='r' encoding='UTF-8'>\n",
      "Found 991 word vectors.\n",
      "-------start_index------------\n",
      "0\n",
      "-------end_index------------\n",
      "50\n",
      "torch.Size([50, 2000])\n",
      "Saving the obtained representations....\n",
      "uisng2.772806406021118s\n"
     ]
    }
   ],
   "source": [
    "#test in real world\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.includes import *\n",
    "\n",
    "\n",
    "home_path = '/home/swj/VD/'\n",
    "code_type = 'C++'\n",
    "data_set_name = 'VLC'\n",
    "#CVE-2021-25804 : OK\n",
    "#CVE-2021-38114 : OK\n",
    "CVE_code = 'CVE-2021-25803'\n",
    "data_path = home_path + 'data/' + 'TEST/' + code_type + '/' + data_set_name + '/'\n",
    "print(data_path)\n",
    "model_name = 'dlstm'\n",
    "ways = 'myway'\n",
    "\n",
    "model_path = '/home/swj/VD/trained_model/' + data_set_name + '/' + model_name + '_' + ways + '_model.h5'\n",
    "tokenizer_saved_path = '/home/swj/VD/embedding/'\n",
    "#change this\n",
    "all_data_path = home_path + 'data/Six_projects/'\n",
    "\n",
    "\n",
    "all_data_list, all_data_list_id = loadData(all_data_path)\n",
    "total_list, total_list_id = loadData(data_path)\n",
    "print(len(total_list))\n",
    "verbose(\"Perform tokenization ....\")\n",
    "Tokenization(all_data_list)\n",
    "verbose(\"Tokenization completed!\")\n",
    "verbose(\"-------------------------------------------------------\")   \n",
    "verbose(\"Perform code embedding ....\")\n",
    "embedding_model = Embedding_Model(config)\n",
    "'''total_sequnces: variable lengths of number of samples vectors'''\n",
    "'''word_index: a map of word to vectors'''\n",
    "total_sequences, word_index = embedding_model.LoadTokenizer(total_list)\n",
    "embedding_model.TrainWordToVec(total_list)\n",
    "embedding_matrix, embedding_dim = embedding_model.ApplyWordToVec(word_index)\n",
    "weights=torch.from_numpy(embedding_matrix)\n",
    "\n",
    "#obtain representations\n",
    "def ObtainRepresentations_by_batch_size(input_sequences, feature_model, BATCH_SIZE):\n",
    "    num_batches_per_epoch = int((len(input_sequences) - 1) / BATCH_SIZE) + 1\n",
    "    data_size = len(input_sequences)\n",
    "    representations_total = []\n",
    "    model = feature_model\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index = batch_num * BATCH_SIZE\n",
    "        end_index = min((batch_num + 1) * BATCH_SIZE, data_size)\n",
    "        print (\"-------start_index------------\")\n",
    "        print (start_index)\n",
    "        print (\"-------end_index------------\")\n",
    "        print (end_index)\n",
    "        print(input_sequences[start_index: end_index].shape)\n",
    "        representations = model(input_sequences[start_index: end_index])\n",
    "        representations_total = representations_total + representations.tolist()\n",
    "    return np.asarray(representations_total)\n",
    "\n",
    "\n",
    "if model_name == 'blstm':\n",
    "    net = BLSTMnet(weights).to(device)\n",
    "elif model_name == 'dlstm':\n",
    "    net = BPLSTMnet(weights).to(device)\n",
    "\n",
    "    \n",
    "#change this\n",
    "\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net.eval()\n",
    "\n",
    "feature_model = None\n",
    "if model_name == 'blstm':\n",
    "    feature_model = BLSTM_Feature_Net(net, weights).to(device)\n",
    "elif model_name == 'dlstm':\n",
    "    feature_model = BPLSTM_Feature_Net(net, weights).to(device)\n",
    "\n",
    "feature_model.eval()\n",
    "tokenizer = LoadToknizer(tokenizer_saved_path + 'tokenizer.pickle')\n",
    "\n",
    "\n",
    "# if vulnerability == 'vul':\n",
    "#     data_list, data_list_id = loadData(data_path + 'Vulnerable_functions/')\n",
    "# else: \n",
    "#     data_list, data_list_id = loadData(data_path + 'Non_vulnerable_functions/')\n",
    "\n",
    "data_list, data_list_id = getCFilesFromText(data_path)\n",
    "data_sequence = tokenizer.texts_to_sequences(data_list)\n",
    "data_list_pad = padding(data_sequence)\n",
    "data_list_pad = torch.Tensor(data_list_pad).long().to(device)\n",
    "start_time = time.time()\n",
    "obtained_repre = ObtainRepresentations_by_batch_size(data_list_pad, feature_model, 128)\n",
    "end_time = time.time()\n",
    "print(\"Saving the obtained representations....\") \n",
    "print('uisng' + str(end_time - start_time) + 's')\n",
    "# if not os.path.exists(repre_saved_path): os.makedirs(repre_saved_path)\n",
    "\n",
    "# if vulnerability == 'vul':\n",
    "#     SavedPickle(repre_saved_path + \"Vul.pkl\", obtained_repre)\n",
    "# else:\n",
    "#     SavedPickle(repre_saved_path + \"NonVul.pkl\", obtained_repre)\n",
    "\n",
    "# print(\"The obtained representations are saved in: \" + str(repre_saved_path) + \".\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4413bb-2071-41fc-8713-478b317a82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVE-2015-5949.c: 1\n",
      "CVE-2021-25803.c: 0\n",
      "CVE-2017-8311.c: 1\n",
      "CVE-2011-3623-1.c: 1\n",
      "CVE-2010-2937.c: 1\n",
      "CVE-2013-1954.c: 1\n",
      "CVE-2011-1684 .c: 1\n",
      "CVE-2021-25801.c: 1\n",
      "CVE-2008-5276.c: 1\n",
      "CVE-2008-4654.c: 1\n",
      "CVE-2018-11516.c: 0\n",
      "CVE-2011-2588.c: 1\n",
      "CVE-2011-2587.c: 1\n",
      "CVE-2008-5036.c: 1\n",
      "CVE-2011-0522-1.c: 1\n",
      "CVE-2008-1881.c: 1\n",
      "CVE-2009-2484.c: 0\n",
      "CVE-2014-1684.c: 1\n",
      "CVE-2017-8310.c: 1\n",
      "CVE-2007-0017-3.c: 1\n",
      "CVE-2017-8313.c: 1\n",
      "CVE-2007-0017-2.c: 1\n",
      "CVE-2010-1443.c: 1\n",
      "CVE-2010-3907-2.c: 1\n",
      "CVE-2014-9598.c: 1\n",
      "CVE-2008-3732.c: 0\n",
      "CVE-2013-4388.c: 1\n",
      "avi.c: 0\n",
      "CVE-2014-9597.c: 1\n",
      "CVE-2016-3941.c: 1\n",
      "CVE-2011-3623-2.c: 1\n",
      "CVE-2010-3124.c: 1\n",
      "CVE-2014-9743.c: 0\n",
      "CVE-2007-0017-1.c: 1\n",
      "CVE-2011-3623-3.c: 1\n",
      "CVE-2010-3907-1.c: 1\n",
      "CVE-2021-38114.c: 0\n",
      "CVE-2018-19857.c: 0\n",
      "CVE-2008-2430.c: 1\n",
      "CVE-2010-1444.c: 1\n",
      "CVE-2011-0522-2.c: 1\n",
      "CVE-2012-0023.c: 1\n",
      "CVE-2016-5108.c: 1\n",
      "CVE-2007-6682.c: 1\n",
      "CVE-2010-2062.c: 1\n",
      "CVE-2008-0984 .c: 1\n",
      "CVE-2021-25802.c: 0\n",
      "CVE-2008-1489.c: 1\n",
      "CVE-2021-25804.c: 0\n",
      "CVE-2008-3794.c: 1\n",
      "# of vul detected: 40\n"
     ]
    }
   ],
   "source": [
    "model_path = home_path + 'trained_model' + '/' + model_name + '_' + data_set_name + '_' + ways + '_RF.joblib'\n",
    "clf = joblib.load(model_path)\n",
    "y_predict = clf.predict(obtained_repre)\n",
    "for i in range(len(y_predict)):\n",
    "    print(data_list_id[i] + ': ' + str(y_predict[i]))\n",
    "print('# of vul detected: ' + str(sum(y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a97188-d124-405e-81e0-12ed33be74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = home_path + 'trained_model' + '/' + 'BLSTM_RF.joblib'\n",
    "clf = joblib.load(model_path)\n",
    "y_predict = clf.predict(obtained_repre)\n",
    "for i in range(len(y_predict)):\n",
    "    print(data_list_id[i] + ': ' + str(y_predict[i]))\n",
    "print('# of vul detected: ' + str(sum(y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31d7e6-141d-479a-83e8-452e83f394c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
