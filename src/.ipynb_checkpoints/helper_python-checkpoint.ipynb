{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa214d-72b5-4003-aa06-d8a3aff013f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'bplstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"command_injection\"\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "\n",
    "#get the vulnerability from the command line argument\n",
    "\n",
    "\n",
    "progress = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "### paramters for the filtering and creation of samples\n",
    "#restriction = [20000,5,6,10] #which samples to filter out\n",
    "restriction = [10000,5,3,5] #which samples to filter out\n",
    "\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "\n",
    "#get word2vec model\n",
    "w2v = embedding_path + \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s)\n",
    "w2vmodel = w2v + \".model\"\n",
    "\n",
    "#load word2vec model\n",
    "if not (os.path.isfile(w2vmodel)):\n",
    "    print(\"word2vec model is still being created...\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec.load(w2vmodel)\n",
    "print('word2vec model loaded!')\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "#load data\n",
    "with open(data_path + 'plain_' + mode, 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "    \n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"finished loading. \", nowformat)\n",
    "\n",
    "allblocks = []\n",
    "\n",
    "for r in data:\n",
    "    progress = progress + 1\n",
    "\n",
    "    for c in data[r]:\n",
    "\n",
    "        if \"files\" in data[r][c]:                      \n",
    "            if len(data[r][c][\"files\"]) > restriction[3]:\n",
    "                #too many files\n",
    "                continue\n",
    "\n",
    "            for f in data[r][c][\"files\"]:\n",
    "\n",
    "                if len(data[r][c][\"files\"][f][\"changes\"]) >= restriction[2]:\n",
    "                    #too many changes in a single file\n",
    "                    continue\n",
    "\n",
    "                if not \"source\" in data[r][c][\"files\"][f]:\n",
    "                    #no sourcecode\n",
    "                    continue\n",
    "\n",
    "                if \"source\" in data[r][c][\"files\"][f]:\n",
    "                    sourcecode = data[r][c][\"files\"][f][\"source\"]                          \n",
    "                    if len(sourcecode) > restriction[0]:\n",
    "                        #sourcecode is too long\n",
    "                        continue\n",
    "\n",
    "                allbadparts = []\n",
    "\n",
    "                for change in data[r][c][\"files\"][f][\"changes\"]:\n",
    "\n",
    "                    #get the modified or removed parts from each change that happened in the commit                  \n",
    "                    badparts = change[\"badparts\"]\n",
    "                    count = count + len(badparts)\n",
    "\n",
    "                    if len(badparts) > restriction[1]:\n",
    "                        #too many modifications in one change\n",
    "                        break\n",
    "\n",
    "                    for bad in badparts:\n",
    "                        #check if they can be found within the file\n",
    "                        pos = myutils.findposition(bad,sourcecode)\n",
    "                        if not -1 in pos:\n",
    "                            allbadparts.append(bad)\n",
    "\n",
    "                    if (len(allbadparts) > restriction[2]):\n",
    "                        #too many bad positions in the file\n",
    "                        break\n",
    "\n",
    "                if(len(allbadparts) > 0):\n",
    "                    if len(allbadparts) < restriction[2]:\n",
    "                        #find the positions of all modified parts\n",
    "                        positions = myutils.findpositions(allbadparts,sourcecode)\n",
    "\n",
    "                        #get the file split up in samples\n",
    "                        blocks = myutils.getblocks(sourcecode, positions, step, fulllength)\n",
    "\n",
    "                        for b in blocks:\n",
    "                          #each is a tuple of code and label\n",
    "                            allblocks.append(b)\n",
    "\n",
    "keys = []\n",
    "\n",
    "#randomize the sample and split into train, validate and final test set\n",
    "for i in range(len(allblocks)):\n",
    "    keys.append(i)\n",
    "random.shuffle(keys)\n",
    "\n",
    "print('there are total ' + str(len(keys)) + ' files')\n",
    "cutoff = round(0.7 * len(keys)) #     70% for the training set\n",
    "cutoff2 = round(0.85 * len(keys)) #   15% for the validation set and 15% for the final test set\n",
    "\n",
    "keystrain = keys[:cutoff]\n",
    "keystest = keys[cutoff:cutoff2]\n",
    "keysfinaltest = keys[cutoff2:]\n",
    "\n",
    "print(\"cutoff \" + str(cutoff))\n",
    "print(\"cutoff2 \" + str(cutoff2))\n",
    "\n",
    "\n",
    "with open(data_path + mode + '_dataset_keystrain', 'wb') as fp:\n",
    "    pickle.dump(keystrain, fp)\n",
    "with open(data_path + mode + '_dataset_keystest', 'wb') as fp:\n",
    "    pickle.dump(keystest, fp)\n",
    "with open(data_path + mode + '_dataset_keysfinaltest', 'wb') as fp:\n",
    "    pickle.dump(keysfinaltest, fp)\n",
    "\n",
    "TrainX = []\n",
    "TrainY = []\n",
    "ValidateX = []\n",
    "ValidateY = []\n",
    "FinaltestX = []\n",
    "FinaltestY = []\n",
    "\n",
    "\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "codes, lables = [], []\n",
    "for code, lable in allblocks:\n",
    "    codes.append(code.split(' '))\n",
    "    lables.append(lable)\n",
    "Tokenization(codes)\n",
    "total_sequences, word_index = LoadTokenizer(codes)\n",
    "word_vectors = w2v_model.wv\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))#word vectors has dimention 200\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_vectors[words[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "weights=torch.from_numpy(embedding_matrix)\n",
    "with open(embedding_path + mode + '_weights', 'wb') as fp:\n",
    "    pickle.dump(weights, fp)\n",
    "    \n",
    "max_length = fulllength \n",
    "total_sequences = pad_sequences(total_sequences, maxlen=max_length)\n",
    "\n",
    "print(total_sequences.shape)\n",
    "print(\"Creating training dataset... (\" + mode + \")\")\n",
    "for k in keystrain:\n",
    "    TrainX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    TrainY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Train length: \" + str(len(TrainX)))\n",
    "print('saving datasets')\n",
    "with open(data_path + 'plain_' + mode + '_dataset-train-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(TrainX, fp)\n",
    "with open(data_path + 'plain_' + mode + '_dataset-train-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(TrainY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"Creating validation dataset...\")\n",
    "for k in keystest:\n",
    "    ValidateX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    ValidateY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Test length: \" + str(len(ValidateX)))\n",
    "with open(data_path + 'plain_' + mode + '_dataset-validate-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(ValidateX, fp)\n",
    "with open(data_path + 'plain_' + mode + '_dataset-validate-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2, 'wb') as fp:\n",
    "    pickle.dump(ValidateY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"Creating finaltest dataset...\")\n",
    "for k in keysfinaltest:\n",
    "    FinaltestX.append(total_sequences[k]) #append the list of vectors to the X (independent variable)\n",
    "    FinaltestY.append(lables[k]) #append the label to the Y (dependent variable)\n",
    "print(\"Finaltesting length: \" + str(len(FinaltestX)))\n",
    "with open(data_path + mode + '_dataset_finaltest_X', 'wb') as fp:\n",
    "    pickle.dump(FinaltestX, fp)\n",
    "with open(data_path + mode + '_dataset_finaltest_Y', 'wb') as fp:\n",
    "    pickle.dump(FinaltestY, fp)\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"time: \", nowformat)\n",
    "\n",
    "print(\"saved dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009681ce-3b41-4138-986f-87a6c71d5682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array done.  11:13\n",
      "8259 samples in the training set.\n",
      "1770 samples in the validation set.\n",
      "percentage of vulnerable samples: 5.59%\n",
      "absolute amount of vulnerable samples in test set: 84\n",
      "Starting DPLSTM:  11:13\n",
      "(8259, 200) (1770, 200)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *#Prepare the data for models\n",
    "model_name = 'plstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"remote_code_execution\"\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "### paramters for the filtering and creation of samples\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "Train_X, Train_y, ValidateX, ValidateY = LoadPickleData(data_path + 'plain_' + mode + '_dataset-train-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-train-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-validate-X_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2), \\\n",
    "LoadPickleData(data_path + 'plain_' + mode + '_dataset-validate-Y_'+ \"word2vec_\"+w+str(mincount) + \"-\" + str(iterationen) +\"-\" + str(s) + \"__\" + mode2)\n",
    "\n",
    "X_train =  np.array(Train_X)\n",
    "y_train =  np.array(Train_y)\n",
    "X_test =  np.array(ValidateX)\n",
    "y_test =  np.array(ValidateY)\n",
    "\n",
    "#in the original collection of data, the 0 and 1 were used the other way round, so now they are switched so that \"1\" means vulnerable and \"0\" means clean.\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 0:\n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = 0\n",
    "    \n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 0:\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"numpy array done. \", nowformat)\n",
    "\n",
    "print(str(len(X_train)) + \" samples in the training set.\")      \n",
    "print(str(len(X_test)) + \" samples in the validation set.\") \n",
    "  \n",
    "csum = 0\n",
    "for a in y_train:\n",
    "    csum = csum+a\n",
    "print(\"percentage of vulnerable samples: \"  + str(int((csum / len(X_train)) * 10000)/100) + \"%\")\n",
    "  \n",
    "testvul = 0\n",
    "for y in y_test:\n",
    "    if y == 1:\n",
    "        testvul = testvul+1\n",
    "print(\"absolute amount of vulnerable samples in test set: \" + str(testvul))\n",
    "\n",
    "  \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "nowformat = now.strftime(\"%H:%M\")\n",
    "print(\"Starting DPLSTM: \", nowformat)\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "train_set_x = torch.Tensor(X_train).long().to(device) # transform to torch tensor\n",
    "train_set_y = torch.Tensor(y_train).to(device)\n",
    "train_dataset = TensorDataset(train_set_x,train_set_y) # create your datset\n",
    "train_loader = DataLoader(train_dataset, batch_size=512) # create your dataloader\n",
    "\n",
    "test_set_x = torch.Tensor(X_test).long().to(device) # transform to torch tensor\n",
    "test_set_y = torch.Tensor(y_test).to(device)\n",
    "test_dataset = TensorDataset(test_set_x,test_set_y) # create your datset\n",
    "test_loader = DataLoader(test_dataset, batch_size=512) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22bf0063-eeaf-4f14-9abc-9f0a45b55ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1 \t\t Training Loss: 0.217149535084472 \t\t Validation Loss: 0.12531010061502457\n",
      "Epoch 0 elapse 16.70642375946045s\n",
      "Epoch 2 \t\t Training Loss: 0.12119379113702212 \t\t Validation Loss: 0.09837939590215683\n",
      "Epoch 1 elapse 16.389198541641235s\n",
      "Epoch 3 \t\t Training Loss: 0.07802276718704139 \t\t Validation Loss: 0.077881402336061\n",
      "Epoch 2 elapse 16.396649837493896s\n",
      "Epoch 4 \t\t Training Loss: 0.04420392763088731 \t\t Validation Loss: 0.05513788480311632\n",
      "Epoch 3 elapse 16.362051486968994s\n",
      "Epoch 5 \t\t Training Loss: 0.03604911725201151 \t\t Validation Loss: 0.05884136166423559\n",
      "Epoch 4 elapse 16.366596460342407s\n",
      "Epoch 6 \t\t Training Loss: 0.030765612938386554 \t\t Validation Loss: 0.03650857601314783\n",
      "Epoch 5 elapse 16.3068745136261s\n",
      "Epoch 7 \t\t Training Loss: 0.023943620569565716 \t\t Validation Loss: 0.027386880479753017\n",
      "Epoch 6 elapse 16.32802391052246s\n",
      "Epoch 8 \t\t Training Loss: 0.018612288455615807 \t\t Validation Loss: 0.0318373953923583\n",
      "Epoch 7 elapse 16.306121110916138s\n",
      "Epoch 9 \t\t Training Loss: 0.016257273122284782 \t\t Validation Loss: 0.0255023380741477\n",
      "Epoch 8 elapse 16.354785919189453s\n",
      "Epoch 10 \t\t Training Loss: 0.013517820563934305 \t\t Validation Loss: 0.021622230298817158\n",
      "Epoch 9 elapse 16.379711627960205s\n",
      "Epoch 11 \t\t Training Loss: 0.013098899494199193 \t\t Validation Loss: 0.03327451925724745\n",
      "Epoch 10 elapse 16.809300899505615s\n",
      "Epoch 12 \t\t Training Loss: 0.013708323637461838 \t\t Validation Loss: 0.02560149901546538\n",
      "Epoch 11 elapse 16.37283229827881s\n",
      "Epoch 13 \t\t Training Loss: 0.010833614889313193 \t\t Validation Loss: 0.02218495379202068\n",
      "Epoch 12 elapse 16.363926887512207s\n",
      "Epoch 14 \t\t Training Loss: 0.009642832830329151 \t\t Validation Loss: 0.016352176666259766\n",
      "Epoch 13 elapse 16.371907711029053s\n",
      "Epoch 15 \t\t Training Loss: 0.015408888828995474 \t\t Validation Loss: 0.018412226461805403\n",
      "Epoch 14 elapse 16.35565710067749s\n",
      "Epoch 16 \t\t Training Loss: 0.014785012908225112 \t\t Validation Loss: 0.015841821557842195\n",
      "Epoch 15 elapse 16.353495121002197s\n",
      "Epoch 17 \t\t Training Loss: 0.010992332245223224 \t\t Validation Loss: 0.030549132265150547\n",
      "Epoch 16 elapse 16.370734930038452s\n",
      "Epoch 18 \t\t Training Loss: 0.006689033763798173 \t\t Validation Loss: 0.021509106154553592\n",
      "Epoch 17 elapse 16.511703729629517s\n",
      "Epoch 19 \t\t Training Loss: 0.010478433579384513 \t\t Validation Loss: 0.024481241824105382\n",
      "Epoch 18 elapse 16.298567295074463s\n",
      "Epoch 20 \t\t Training Loss: 0.007502872020268114 \t\t Validation Loss: 0.025935186073184013\n",
      "Epoch 19 elapse 16.299439191818237s\n",
      "Epoch 21 \t\t Training Loss: 0.005906175798051716 \t\t Validation Loss: 0.024464931106194854\n",
      "Epoch 20 elapse 16.268961191177368s\n",
      "Epoch 22 \t\t Training Loss: 0.004401874175528064 \t\t Validation Loss: 0.020808838147786446\n",
      "Epoch 21 elapse 16.21806240081787s\n",
      "Epoch 23 \t\t Training Loss: 0.007539070038225926 \t\t Validation Loss: 0.023337917285971344\n",
      "Epoch 22 elapse 16.25346088409424s\n",
      "Epoch 24 \t\t Training Loss: 0.005919248031739912 \t\t Validation Loss: 0.014051341451704502\n",
      "Epoch 23 elapse 16.22962760925293s\n",
      "Epoch 25 \t\t Training Loss: 0.005433962617055787 \t\t Validation Loss: 0.02166259300429374\n",
      "Epoch 24 elapse 16.22347331047058s\n",
      "Epoch 26 \t\t Training Loss: 0.006422442031394252 \t\t Validation Loss: 0.017951331799849868\n",
      "Epoch 25 elapse 16.28610324859619s\n",
      "Epoch 27 \t\t Training Loss: 0.004869529736906235 \t\t Validation Loss: 0.01967346464516595\n",
      "Epoch 26 elapse 16.2160325050354s\n",
      "Epoch 28 \t\t Training Loss: 0.006431054275119951 \t\t Validation Loss: 0.019723539240658283\n",
      "Epoch 27 elapse 16.218235731124878s\n",
      "Epoch 29 \t\t Training Loss: 0.003476679899476414 \t\t Validation Loss: 0.0245589999249205\n",
      "Epoch 28 elapse 16.214048147201538s\n",
      "Epoch 30 \t\t Training Loss: 0.0035828770633088425 \t\t Validation Loss: 0.025422275983146392\n",
      "Epoch 29 elapse 16.219648361206055s\n",
      "Epoch 31 \t\t Training Loss: 0.005292940080193255 \t\t Validation Loss: 0.0228676307015121\n",
      "Epoch 30 elapse 16.2127628326416s\n",
      "Epoch 32 \t\t Training Loss: 0.0056429562502992615 \t\t Validation Loss: 0.025839435402303934\n",
      "Epoch 31 elapse 16.271525859832764s\n",
      "Epoch 33 \t\t Training Loss: 0.003996058648922104 \t\t Validation Loss: 0.03213808708824217\n",
      "Epoch 32 elapse 16.24216389656067s\n",
      "Epoch 34 \t\t Training Loss: 0.006678574989029371 \t\t Validation Loss: 0.026244804495945573\n",
      "Epoch 33 elapse 16.21769618988037s\n",
      "Epoch 35 \t\t Training Loss: 0.003655221725399743 \t\t Validation Loss: 0.03125215694308281\n",
      "Epoch 34 elapse 16.215039491653442s\n",
      "Epoch 36 \t\t Training Loss: 0.002710606746511664 \t\t Validation Loss: 0.025699370773509145\n",
      "Epoch 35 elapse 16.22390127182007s\n",
      "Epoch 37 \t\t Training Loss: 0.0015129943210066652 \t\t Validation Loss: 0.031197911710478365\n",
      "Epoch 36 elapse 16.215755701065063s\n",
      "Epoch 38 \t\t Training Loss: 0.002854759441668327 \t\t Validation Loss: 0.025736747542396188\n",
      "Epoch 37 elapse 16.231186389923096s\n",
      "Epoch 39 \t\t Training Loss: 0.002599556398255278 \t\t Validation Loss: 0.02361773489974439\n",
      "Epoch 38 elapse 16.22326922416687s\n",
      "Epoch 40 \t\t Training Loss: 0.00193252031417882 \t\t Validation Loss: 0.02272826968692243\n",
      "Epoch 39 elapse 16.242480516433716s\n",
      "Epoch 41 \t\t Training Loss: 0.0009532480340995415 \t\t Validation Loss: 0.02754225698299706\n",
      "Epoch 40 elapse 16.249457836151123s\n",
      "Epoch 42 \t\t Training Loss: 0.004277646221667323 \t\t Validation Loss: 0.02789605502039194\n",
      "Epoch 41 elapse 16.298529624938965s\n",
      "Epoch 43 \t\t Training Loss: 0.004568626613678027 \t\t Validation Loss: 0.0249480027705431\n",
      "Epoch 42 elapse 16.23674964904785s\n",
      "Epoch 44 \t\t Training Loss: 0.0034087371934540846 \t\t Validation Loss: 0.02663572644814849\n",
      "Epoch 43 elapse 16.234703063964844s\n",
      "Epoch 45 \t\t Training Loss: 0.004473889739636113 \t\t Validation Loss: 0.030989482766017318\n",
      "Epoch 44 elapse 16.300097703933716s\n",
      "Epoch 46 \t\t Training Loss: 0.002858040848663864 \t\t Validation Loss: 0.022036357782781124\n",
      "Epoch 45 elapse 16.353015661239624s\n",
      "Epoch 47 \t\t Training Loss: 0.001656612044933033 \t\t Validation Loss: 0.023455025861039758\n",
      "Epoch 46 elapse 16.345975637435913s\n",
      "Epoch 48 \t\t Training Loss: 0.0014815355629359734 \t\t Validation Loss: 0.025361872394569218\n",
      "Epoch 47 elapse 16.491392135620117s\n",
      "Epoch 49 \t\t Training Loss: 0.0015139520507866965 \t\t Validation Loss: 0.03310867724940181\n",
      "Epoch 48 elapse 16.23099708557129s\n",
      "Epoch 50 \t\t Training Loss: 0.001655544117484704 \t\t Validation Loss: 0.027214991685468704\n",
      "Epoch 49 elapse 16.224569082260132s\n",
      "Epoch 51 \t\t Training Loss: 0.004341538962976569 \t\t Validation Loss: 0.03669460630044341\n",
      "Epoch 50 elapse 16.232255697250366s\n",
      "Epoch 52 \t\t Training Loss: 0.011198281509089558 \t\t Validation Loss: 0.03078429913148284\n",
      "Epoch 51 elapse 16.23475670814514s\n",
      "Epoch 53 \t\t Training Loss: 0.008831204961204682 \t\t Validation Loss: 0.027559179114177823\n",
      "Epoch 52 elapse 16.226309061050415s\n",
      "Epoch 54 \t\t Training Loss: 0.007118567600093015 \t\t Validation Loss: 0.02828297985251993\n",
      "Epoch 53 elapse 16.23193621635437s\n",
      "Epoch 55 \t\t Training Loss: 0.004431343842845629 \t\t Validation Loss: 0.021051169838756323\n",
      "Epoch 54 elapse 16.45434856414795s\n",
      "Epoch 56 \t\t Training Loss: 0.0025265687607530065 \t\t Validation Loss: 0.01890285173431039\n",
      "Epoch 55 elapse 16.364821195602417s\n",
      "Epoch 57 \t\t Training Loss: 0.0031489541852886974 \t\t Validation Loss: 0.01119104283861816\n",
      "Epoch 56 elapse 16.218401193618774s\n",
      "Epoch 58 \t\t Training Loss: 0.001963438595714037 \t\t Validation Loss: 0.024385084863752127\n",
      "Epoch 57 elapse 16.229191064834595s\n",
      "Epoch 59 \t\t Training Loss: 0.0019237121820397613 \t\t Validation Loss: 0.02449512481689453\n",
      "Epoch 58 elapse 16.216897010803223s\n",
      "Epoch 60 \t\t Training Loss: 0.001884325078850345 \t\t Validation Loss: 0.027266987483017147\n",
      "Epoch 59 elapse 16.221843242645264s\n",
      "    epoch  train_loss  valid_loss\n",
      "0       0    3.691542    0.501240\n",
      "1       1    2.060294    0.393518\n",
      "2       2    1.326387    0.311526\n",
      "3       3    0.751467    0.220552\n",
      "4       4    0.612835    0.235365\n",
      "5       5    0.523015    0.146034\n",
      "6       6    0.407042    0.109548\n",
      "7       7    0.316409    0.127350\n",
      "8       8    0.276374    0.102009\n",
      "9       9    0.229803    0.086489\n",
      "10     10    0.222681    0.133098\n",
      "11     11    0.233042    0.102406\n",
      "12     12    0.184171    0.088740\n",
      "13     13    0.163928    0.065409\n",
      "14     14    0.261951    0.073649\n",
      "15     15    0.251345    0.063367\n",
      "16     16    0.186870    0.122197\n",
      "17     17    0.113714    0.086036\n",
      "18     18    0.178133    0.097925\n",
      "19     19    0.127549    0.103741\n",
      "20     20    0.100405    0.097860\n",
      "21     21    0.074832    0.083235\n",
      "22     22    0.128164    0.093352\n",
      "23     23    0.100627    0.056205\n",
      "24     24    0.092377    0.086650\n",
      "25     25    0.109182    0.071805\n",
      "26     26    0.082782    0.078694\n",
      "27     27    0.109328    0.078894\n",
      "28     28    0.059104    0.098236\n",
      "29     29    0.060909    0.101689\n",
      "30     30    0.089980    0.091471\n",
      "31     31    0.095930    0.103358\n",
      "32     32    0.067933    0.128552\n",
      "33     33    0.113536    0.104979\n",
      "34     34    0.062139    0.125009\n",
      "35     35    0.046080    0.102797\n",
      "36     36    0.025721    0.124792\n",
      "37     37    0.048531    0.102947\n",
      "38     38    0.044192    0.094471\n",
      "39     39    0.032853    0.090913\n",
      "40     40    0.016205    0.110169\n",
      "41     41    0.072720    0.111584\n",
      "42     42    0.077667    0.099792\n",
      "43     43    0.057949    0.106543\n",
      "44     44    0.076056    0.123958\n",
      "45     45    0.048587    0.088145\n",
      "46     46    0.028162    0.093820\n",
      "47     47    0.025186    0.101447\n",
      "48     48    0.025737    0.132435\n",
      "49     49    0.028144    0.108860\n",
      "50     50    0.073806    0.146778\n",
      "51     51    0.190371    0.123137\n",
      "52     52    0.150130    0.110237\n",
      "53     53    0.121016    0.113132\n",
      "54     54    0.075333    0.084205\n",
      "55     55    0.042952    0.075611\n",
      "56     56    0.053532    0.044764\n",
      "57     57    0.033378    0.097540\n",
      "58     58    0.032703    0.097980\n",
      "59     59    0.032034    0.109068\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'dlstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "code_type = 'python'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "mode = \"remote_code_execution\"\n",
    "model_path = home_path + 'trained_model/' + code_type + '_' + mode + '/'\n",
    "\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "recording_type = 'loss&acc'\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "#torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "print(device)\n",
    "\n",
    "weights=LoadPickleData(embedding_path + mode + '_weights')\n",
    "\n",
    "dlstm = DLSTM_python_net(weights).to(device)\n",
    "plstm = PLSTM_python_net(weights).to(device)\n",
    "blstm = BLSTM_python_net(weights).to(device)\n",
    "lstm = LSTMnet(weights).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = None\n",
    "if model_name == 'blstm':\n",
    "    optimizer = optim.Adam(blstm.parameters(), lr=0.001)\n",
    "elif model_name == 'dlstm':\n",
    "    optimizer = optim.Adam(dlstm.parameters(), lr=0.001)\n",
    "elif model_name == 'plstm':\n",
    "    optimizer = optim.Adam(plstm.parameters(), lr=0.001)\n",
    "elif model_name == 'lstm':\n",
    "    optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 60\n",
    "is_first = True\n",
    "def train(model, device, train_loader, test_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        valid_loss += loss.item() \n",
    "        valid_acc += acc.item() \n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(test_loader)}')\n",
    "    return train_loss, train_acc, valid_loss, valid_acc\n",
    "\n",
    "def D_train(model, device, train_loader, test_loader, optimizer, epoch, A):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, A)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "    \n",
    "    model.eval()     # Optional when not using Model Specific layer\n",
    "    valid_loss, valid_acc = 0, 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data, A)\n",
    "        loss = criterion(output, target.unsqueeze(1))\n",
    "        acc = binary_acc(output, target.unsqueeze(1))\n",
    "        valid_loss += loss.item() \n",
    "        valid_acc += acc.item() \n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(test_loader)}')\n",
    "    return train_loss, train_acc, valid_loss, valid_acc\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=home_path + 'src/runs/' + model_name + '_' + code_type + '_' + mode + '/', comment=model_name + '_' + code_type + '_' + mode)\n",
    "csv_writer = []\n",
    "\n",
    "ts = torch.rand(train_set_x.shape, dtype=torch.double).to(device)\n",
    "domain_set = len(train_set_x) * [1] + len(ts) * [0]\n",
    "A = torch.eye(200, dtype = train_set_x.dtype).to(device)\n",
    "for epoch in range(epochs):\n",
    "    train_set_x.to(device)\n",
    "    train_set_x = torch.tensor(train_set_x, dtype=torch.double)\n",
    "    A = Newton(torch.cat((train_set_x, ts), dim = 0), domain_set, 4)\n",
    "    ts = ts @ A.sqrt()\n",
    "\n",
    "    time1 = time.time()\n",
    "    if model_name == 'blstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(blstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    elif model_name == 'dlstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(dlstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    elif model_name == 'plstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = D_train(plstm, device, train_loader, test_loader, optimizer, epoch, A)\n",
    "    elif model_name == 'lstm':\n",
    "        train_loss, train_acc, valid_loss, valid_acc = train(lstm, device, train_loader, test_loader, optimizer, epoch)\n",
    "    time2 = time.time()\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Loss/valid', valid_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/valid', valid_acc, epoch)\n",
    "    csv_writer.append([epoch, train_loss, valid_loss])\n",
    "    print(f'Epoch {epoch} elapse {time2 - time1}s')\n",
    "\n",
    "Columns = ['epoch','train_loss','valid_loss']\n",
    "CSV_writer=pd.DataFrame(columns=Columns,data=csv_writer)\n",
    "print(CSV_writer)\n",
    "csv_saved_path = home_path + 'results/' + model_name + '/' + code_type + '/' + mode + '/'\n",
    "if not os.path.exists(csv_saved_path): os.makedirs(csv_saved_path)\n",
    "CSV_writer.to_csv(csv_saved_path + model_name + '_' + mode + '.csv',encoding='gbk')\n",
    "\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "if model_name == 'blstm':\n",
    "    torch.save(blstm.state_dict(),  model_path + model_name + '_model.h5')\n",
    "elif model_name == 'dlstm':\n",
    "    torch.save(dlstm.state_dict(), model_path + model_name + '_model.h5')\n",
    "elif model_name == 'lstm':\n",
    "    torch.save(lstm.state_dict(), model_path + model_name + '_model.h5')\n",
    "elif model_name == 'plstm':\n",
    "    torch.save(plstm.state_dict(), model_path + model_name + '_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4877928-f43b-4ad8-b56a-aea95e27ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.includes import *\n",
    "\n",
    "\n",
    "model_name = 'plstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "code_type = 'python'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "mode = \"remote_code_execution\"\n",
    "ways = 'myway'\n",
    "model_path = home_path + 'trained_model/' + code_type + '_' + mode + '/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "data_path = home_path + 'data/python_data_set/' + mode + '/'\n",
    "repre_saved_path = home_path + 'Representation/' + model_name + '_' + ways + '/' + code_type + '/' + mode + '/'\n",
    "\n",
    "### hyperparameters for the w2v model\n",
    "mincount = 10 #minimum times a word has to appear in the corpus to be in the word2vec model\n",
    "iterationen = 300 #training iterations for the word2vec model\n",
    "s = 200 #dimensions of the word2vec model\n",
    "w = \"withString\" #word2vec model is not replacing strings but keeping them\n",
    "### paramters for the filtering and creation of samples\n",
    "restriction = [20000,5,6,10] #which samples to filter out\n",
    "step = 5 #step lenght n in the description\n",
    "fulllength = 200 #context length m in the description\n",
    "mode2 = str(step)+\"_\"+str(fulllength) \n",
    "\n",
    "Test_X, Test_y = LoadPickleData(data_path + mode + '_dataset_finaltest_X'), LoadPickleData(data_path + mode + '_dataset_finaltest_Y')\n",
    "\n",
    "X_test =  np.array(Test_X)\n",
    "X_test = torch.Tensor(X_test).long().to(device) # transform to torch tensor\n",
    "\n",
    "    \n",
    "for i in range(len(Test_y)):\n",
    "    if Test_y[i] == 0:\n",
    "        Test_y[i] = 1\n",
    "    else:\n",
    "        Test_y[i] = 0\n",
    "        \n",
    "#Train_X_repre, Train_y_repre, Test_X_repre, Test_y_repre = [], [], [], []\n",
    "#obtain representations\n",
    "def ObtainRepresentations_by_batch_size(input_sequences, feature_model, BATCH_SIZE):\n",
    "    num_batches_per_epoch = int((len(input_sequences) - 1) / BATCH_SIZE) + 1\n",
    "    data_size = len(input_sequences)\n",
    "    representations_total = []\n",
    "    model = feature_model\n",
    "    for batch_num in range(num_batches_per_epoch):\n",
    "        start_index = batch_num * BATCH_SIZE\n",
    "        end_index = min((batch_num + 1) * BATCH_SIZE, data_size)\n",
    "        print (\"-------start_index------------\")\n",
    "        print (start_index)\n",
    "        print (\"-------end_index------------\")\n",
    "        print (end_index)\n",
    "        print(input_sequences[start_index: end_index].shape)\n",
    "        representations = model(input_sequences[start_index: end_index])\n",
    "        representations_total = representations_total + representations.tolist()\n",
    "    return np.asarray(representations_total)\n",
    "\n",
    "weights=LoadPickleData(embedding_path + mode + '_weights')\n",
    "\n",
    "if model_name == 'dlstm':\n",
    "    net = DLSTM_python_net(weights).to(device)\n",
    "elif model_name == 'plstm':\n",
    "    net = PLSTM_python_net(weights).to(device)\n",
    "elif model_name == 'blstm':\n",
    "    net = BLSTM_python_net(weights).to(device)\n",
    "    \n",
    "#change this\n",
    "\n",
    "net.load_state_dict(torch.load(model_path + model_name + '_model.h5'))\n",
    "net.eval()\n",
    "\n",
    "feature_model = None\n",
    "if model_name == 'blstm':\n",
    "    feature_model = BLSTM_python_Feature_Net(net, weights).to(device)\n",
    "elif model_name == 'dlstm':\n",
    "    feature_model = DLSTM_python_Feature_Net(net, weights).to(device)\n",
    "elif model_name == 'plstm':\n",
    "    feature_model = PLSTM_python_Feature_Net(net, weights).to(device)\n",
    "    \n",
    "feature_model.eval()\n",
    "start_time = time.time()\n",
    "if not os.path.exists(repre_saved_path): os.makedirs(repre_saved_path)\n",
    "\n",
    "obtained_repre = ObtainRepresentations_by_batch_size(X_test, feature_model, 512)\n",
    "SavedPickle(repre_saved_path + \"X_test.pkl\", obtained_repre)\n",
    "SavedPickle(repre_saved_path + \"y_test.pkl\", Test_y)\n",
    "print(\"Saving the obtained representations....\") \n",
    "end_time = time.time()\n",
    "print('uisng' + str(end_time - start_time) + 's')\n",
    "print(\"The obtained representations are saved in: \" + str(repre_saved_path) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6250ade-61ac-4117-b7fb-d72f13271fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the num of datasets\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.includes import *\n",
    "\n",
    "model_name = 'bplstm'\n",
    "home_path = '/home/swj/VD/'\n",
    "embedding_path = home_path + 'embedding/'\n",
    "\n",
    "#default mode / type of vulnerability\n",
    "mode = \"command_injection\"\n",
    "\n",
    "data_sets_names = ['command_injection', 'open_redirect', 'path_disclosure', 'remote_code_execution', 'sql', 'xsrf', 'xss']\n",
    "for data_sets_name in data_sets_names:\n",
    "    data_path = home_path + 'data/python_data_set/' + data_sets_name + '/'\n",
    "    train = LoadPickleData(data_path + 'plain_' + data_sets_name + '_dataset-train-Y_word2vec_withString10-300-200__5_200')\n",
    "    validation = LoadPickleData(data_path + 'plain_' + data_sets_name + '_dataset-validate-Y_word2vec_withString10-300-200__5_200')\n",
    "    test = LoadPickleData(data_path + data_sets_name + '_dataset_finaltest_Y')\n",
    "    \n",
    "    tot = len(train + validation + test)\n",
    "    nonvul = sum(train + validation + test)\n",
    "    vul = tot - nonvul\n",
    "    proportion = vul / tot\n",
    "    print('the data_set is:  ' + data_sets_name + '    there are   ' + str(tot) + ' total samples,  ' + str(vul) + ' vul samples,  ' + str(nonvul) + ' nonvul samples,   ' + 'the propotion is:   ' + str(vul / tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ae18a-ef1f-4846-b273-d81c78e7159c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
